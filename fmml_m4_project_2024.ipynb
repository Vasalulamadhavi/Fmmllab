{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vasalulamadhavi/Fmmllab/blob/main/fmml_m4_project_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Foundations of Modern Machine Learning, IIIT Hyderabad\n",
        "\n",
        "Project: Perceptron and Gradient Descent\n",
        "\n",
        "Note: Please write the code for the following tasks in separate code cells.  \n"
      ],
      "metadata": {
        "id": "IdS6w29jbANZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBEAaE_RajV3"
      },
      "source": [
        "# Notebook Imports and Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "HR_ZqKsZajV7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
        "from matplotlib import cm # color map\n",
        "\n",
        "from sympy import symbols, diff\n",
        "from math import log\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4WC45IBajV9"
      },
      "source": [
        "### Example 1 - A simple cost function\n",
        "\n",
        "#### $$f(x) = x^2 + x + 1$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Tn4fTS61ajV-"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    return x ** 2 + x + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Q06sqj_HajV-"
      },
      "outputs": [],
      "source": [
        "# Make Data\n",
        "x_1 = np.linspace(start=-3, stop=3, num=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0Nas1x4ajV-"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "plt.xlim(-3, 3)\n",
        "plt.ylim(0, 8)\n",
        "plt.xlabel('X', fontsize=16)\n",
        "plt.ylabel('f(x)', fontsize=16)\n",
        "plt.plot(x_1, f(x_1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVw1B3yBajV_"
      },
      "source": [
        "### Slope & Derivatives\n",
        "\n",
        "Creating a function for the derivative of $f(x)$ called df(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "rThQBc-VajWA"
      },
      "outputs": [],
      "source": [
        "def df(x):\n",
        "    return 2 * x + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHU8OWpkajWB"
      },
      "outputs": [],
      "source": [
        "# Plot function and derivative side by side\n",
        "plt.figure(figsize=[15, 5])\n",
        "\n",
        "# 1 Chart: Cost function\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.xlim(-3, 3)\n",
        "plt.ylim(0, 8)\n",
        "\n",
        "plt.title('Cost function', fontsize=17)\n",
        "plt.xlabel('X', fontsize=16)\n",
        "plt.ylabel('f(x)', fontsize=16)\n",
        "\n",
        "plt.plot(x_1, f(x_1), color='blue', linewidth=3)\n",
        "\n",
        "# 2 Chart: Derivative\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "plt.title('Slope of the cost function', fontsize=17)\n",
        "plt.xlabel('X', fontsize=16)\n",
        "plt.ylabel('df(x)', fontsize=16)\n",
        "plt.grid()\n",
        "plt.xlim(-2, 3)\n",
        "plt.ylim(-3, 6)\n",
        "\n",
        "plt.plot(x_1, df(x_1), color='skyblue', linewidth=5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfAUtAmTajWB"
      },
      "source": [
        "### Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te6xi7U0ajWC"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "##########################\n",
        "\n",
        "## TASK-1 : Complete the lines of code wherever marked as [REQUIRED] in this cell.\n",
        "\n",
        "##########################\n",
        "##########################\n",
        "\n",
        "# Gradient Descent\n",
        "new_x = 3\n",
        "previous_x = 0\n",
        "step_multiplier = 0.1\n",
        "precision = 0.00001\n",
        "\n",
        "x_list = [new_x]\n",
        "slope_list = [df(new_x)]\n",
        "\n",
        "for n in range(500):\n",
        "    previous_x = new_x\n",
        "    gradient = ## [REQUIRED]\n",
        "    new_x = ## [REQUIRED]\n",
        "\n",
        "    step_size = abs(new_x - previous_x)\n",
        "    # print(step_size)\n",
        "\n",
        "    x_list.append(new_x)\n",
        "    slope_list.append(## [REQUIRED])\n",
        "\n",
        "    if step_size < precision:\n",
        "        print('Loop ran this many times:', n)\n",
        "        break\n",
        "\n",
        "print('Local minimum occurs at:', new_x)\n",
        "print('Slope or df(x) value at this point is:', df(new_x))\n",
        "print('f(x) value or cost at this point is:', f(new_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uQFGRESajWD"
      },
      "outputs": [],
      "source": [
        "# Superimpose the gradient descent calculations on plot\n",
        "\n",
        "plt.figure(figsize=[20, 5])\n",
        "\n",
        "# 1 Chart: Cost function\n",
        "plt.subplot(1, 3, 1)\n",
        "\n",
        "plt.xlim(-3, 3)\n",
        "plt.ylim(0, 8)\n",
        "\n",
        "plt.title('Cost function', fontsize=17)\n",
        "plt.xlabel('X', fontsize=16)\n",
        "plt.ylabel('f(x)', fontsize=16)\n",
        "\n",
        "plt.plot(x_1, f(x_1), color='blue', linewidth=3, alpha=0.8)\n",
        "\n",
        "values = np.array(x_list)\n",
        "plt.scatter(x_list, f(values), color='red', s=100, alpha=0.6)\n",
        "\n",
        "# 2 Chart: Derivative\n",
        "plt.subplot(1, 3, 2)\n",
        "\n",
        "plt.title('Slope of the cost function', fontsize=17)\n",
        "plt.xlabel('X', fontsize=16)\n",
        "plt.ylabel('df(x)', fontsize=16)\n",
        "plt.grid()\n",
        "plt.xlim(-2, 3)\n",
        "plt.ylim(-3, 6)\n",
        "\n",
        "plt.plot(x_1, df(x_1), color='skyblue', linewidth=5, alpha=0.6)\n",
        "plt.scatter(x_list, slope_list, color='red', s=100, alpha=0.5)\n",
        "\n",
        "# 3 Chart: Derivative (Close Up)\n",
        "plt.subplot(1, 3, 3)\n",
        "\n",
        "plt.title('Gradient Descent (close up)', fontsize=17)\n",
        "plt.xlabel('X', fontsize=16)\n",
        "plt.grid()\n",
        "plt.xlim(-0.55, -0.2)\n",
        "plt.ylim(-0.3, 0.8)\n",
        "\n",
        "plt.plot(x_1, df(x_1), color='skyblue', linewidth=6, alpha=0.8)\n",
        "plt.scatter(x_list, slope_list, color='red', s=300, alpha=0.6)\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8SA-6tNajWE"
      },
      "source": [
        "### Example 2 - Multiple Minima vs Initial Guess & Advanced Functions\n",
        "\n",
        "#### $$g(x) = x^4 - 4x^2 + 5$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "ZgavFhIYajWE"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "##########################\n",
        "\n",
        "## TASK-2 : Complete the lines of code wherever marked as [REQUIRED] in this cell.\n",
        "\n",
        "##########################\n",
        "##########################\n",
        "\n",
        "# Make some data\n",
        "x_2 = np.linspace(-2, 2, 1000)\n",
        "\n",
        "def g(x):\n",
        "    return ## [REQUIRED]\n",
        "\n",
        "def dg(x):\n",
        "    return ## [REQUIRED]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3mYzGdgajWE"
      },
      "outputs": [],
      "source": [
        "## TASK-3 : Plot function and derivative side by side like has been done earlier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8MhmKKbajWF"
      },
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "4PwmeaFxajWF"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(derivative_func, initial_guess, multiplier=0.02, precision=0.001,\n",
        "                    max_iter=300):\n",
        "    new_x = initial_guess\n",
        "    x_list = [new_x]\n",
        "    slope_list = [derivative_func(new_x)]\n",
        "\n",
        "    for n in range(max_iter):\n",
        "        previous_x = new_x\n",
        "        gradient = derivative_func(previous_x)\n",
        "        new_x = previous_x - multiplier * gradient\n",
        "\n",
        "        step_size = abs(new_x - previous_x)\n",
        "        x_list.append(new_x)\n",
        "        slope_list.append(derivative_func(new_x))\n",
        "\n",
        "        if step_size < precision:\n",
        "            break\n",
        "    return new_x, x_list, slope_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkTnlSfjajWF"
      },
      "outputs": [],
      "source": [
        "local_min, list_x, deriv_list = gradient_descent(dg, 0.5, 0.02, 0.001)\n",
        "print('Local min occurs at:', local_min)\n",
        "print('Number of steps:', len(list_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVNFUXVQajWF"
      },
      "outputs": [],
      "source": [
        "local_min, list_x, deriv_list = gradient_descent(derivative_func=dg, initial_guess= -0.5,\n",
        "                                                 multiplier=0.01, precision=0.0001)\n",
        "print('Local min occurs at:', local_min)\n",
        "print('Number of steps:', len(list_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh8X34SKajWG"
      },
      "outputs": [],
      "source": [
        "local_min, list_x, deriv_list = gradient_descent(derivative_func=dg, initial_guess= -0.1)\n",
        "print('Local min occurs at:', local_min)\n",
        "print('Number of steps:', len(list_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XSGY7FQajWG"
      },
      "outputs": [],
      "source": [
        "# Calling gradient descent function\n",
        "local_min, list_x, deriv_list = gradient_descent(derivative_func=dg, initial_guess= 0)\n",
        "\n",
        "# Plot function and derivative and scatter plot side by side\n",
        "\n",
        "plt.figure(figsize=[15, 5])\n",
        "\n",
        "# 1 Chart: Cost function\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.xlim(-2, 2)\n",
        "plt.ylim(0.5, 5.5)\n",
        "\n",
        "plt.title('Cost function', fontsize=17)\n",
        "plt.xlabel('X', fontsize=16)\n",
        "plt.ylabel('g(x)', fontsize=16)\n",
        "\n",
        "plt.plot(x_2, g(x_2), color='blue', linewidth=3, alpha=0.8)\n",
        "plt.scatter(list_x, g(np.array(list_x)), color='red', s=100, alpha=0.6)\n",
        "\n",
        "# 2 Chart: Derivative\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "plt.title('Slope of the cost function', fontsize=17)\n",
        "plt.xlabel('X', fontsize=16)\n",
        "plt.ylabel('dg(x)', fontsize=16)\n",
        "plt.grid()\n",
        "plt.xlim(-2, 2)\n",
        "plt.ylim(-6, 8)\n",
        "\n",
        "plt.plot(x_2, dg(x_2), color='skyblue', linewidth=5, alpha=0.6)\n",
        "plt.scatter(list_x, deriv_list, color='red', s=100, alpha=0.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neRePpk0ajWG"
      },
      "source": [
        "### Example 3 - Divergence and Overflow\n",
        "\n",
        "#### $$h(x) = x^5 - 2x^4 + 2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "hi1KQGkvajWG"
      },
      "outputs": [],
      "source": [
        "## TASK-4 : Code the same things as they were done for the first two examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5U0O6rnajWH"
      },
      "source": [
        "#### Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTACZznSajWI"
      },
      "outputs": [],
      "source": [
        "# Calling gradient descent function\n",
        "local_min, list_x, deriv_list = gradient_descent(derivative_func=dg, initial_guess= 1.9,\n",
        "                                                multiplier=0.02, max_iter=500)\n",
        "\n",
        "# Plot function and derivative and scatter plot side by side\n",
        "\n",
        "plt.figure(figsize=[15, 5])\n",
        "\n",
        "# 1 Chart: Cost function\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.xlim(-2, 2)\n",
        "plt.ylim(0.5, 5.5)\n",
        "\n",
        "plt.title('Cost function', fontsize=17)\n",
        "plt.xlabel('X', fontsize=16)\n",
        "plt.ylabel('g(x)', fontsize=16)\n",
        "\n",
        "plt.plot(x_2, g(x_2), color='blue', linewidth=3, alpha=0.8)\n",
        "plt.scatter(list_x, g(np.array(list_x)), color='red', s=100, alpha=0.6)\n",
        "\n",
        "# 2 Chart: Derivative\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "plt.title('Slope of the cost function', fontsize=17)\n",
        "plt.xlabel('X', fontsize=16)\n",
        "plt.ylabel('dg(x)', fontsize=16)\n",
        "plt.grid()\n",
        "plt.xlim(-2, 2)\n",
        "plt.ylim(-6, 8)\n",
        "\n",
        "plt.plot(x_2, dg(x_2), color='skyblue', linewidth=5, alpha=0.6)\n",
        "plt.scatter(list_x, deriv_list, color='red', s=100, alpha=0.5)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('Number of steps is: ', len(list_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoQFmI-CajWI"
      },
      "outputs": [],
      "source": [
        "## TASK-5 : Run gradient descent 3 times\n",
        "## TASK-6 : Plot two more learning rates: mid_gamma (0.001) and high_gamma (0.002)\n",
        "n = 100\n",
        "low_gamma = gradient_descent(derivative_func=dg, initial_guess= 3,\n",
        "                                                multiplier=0.0005, precision=0.0001, max_iter=n)\n",
        "\n",
        "\n",
        "mid_gamma = ## [REQUIRED]\n",
        "high_gamma = ## [REQUIRED]\n",
        "\n",
        "# Experiment\n",
        "insane_gamma = gradient_descent(derivative_func=dg, initial_guess= 1.9,\n",
        "                                                multiplier=0.25, precision=0.0001, max_iter=n)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK-7 : Plot reduction in cost for each iteration"
      ],
      "metadata": {
        "id": "Vmv_0RQddmCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGy302BVajWI"
      },
      "source": [
        "### Example 4 - Data Viz with 3D Charts\n",
        "\n",
        "#### Minimise $$f(x, y) = \\frac{1}{3^{-x^2 - y^2} + 1}$$\n",
        "\n",
        "Minimise $$f(x, y) = \\frac{1}{r + 1}$$ where $r$ is $3^{-x^2 - y^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "cV5acLjaajWI"
      },
      "outputs": [],
      "source": [
        "## TASK-8 : Code the same things as they were done for the first two examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUSsuyp7ajWI"
      },
      "outputs": [],
      "source": [
        "# Make our x and y data\n",
        "x_4 = np.linspace(start=-2, stop=2, num=200)\n",
        "y_4 = np.linspace(start=-2, stop=2, num=200)\n",
        "\n",
        "print('Shape of X array', x_4.shape)\n",
        "\n",
        "x_4, y_4 = np.meshgrid(x_4, y_4)\n",
        "print('Array after meshgrid: ', x_4.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYujmRcjajWJ"
      },
      "outputs": [],
      "source": [
        "# Generating 3D Plot\n",
        "fig = plt.figure(figsize=[16, 12])\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "ax.set_xlabel('X', fontsize=20)\n",
        "ax.set_ylabel('Y', fontsize=20)\n",
        "ax.set_zlabel('f(x, y) - Cost', fontsize=20)\n",
        "\n",
        "ax.plot_surface(x_4, y_4, f(x_4, y_4), cmap=cm.coolwarm, alpha=0.4)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQbIOuPWajWJ"
      },
      "source": [
        "### TASK-9 : Complete the lines of text wherever marked as [REQUIRED] in this cell.\n",
        "\n",
        "\n",
        "#### Partial Derivatives & Symbolic Computation\n",
        "\n",
        "#### $$\\frac{\\partial f}{\\partial x} = ## [REQUIRED]\n",
        "\n",
        "#### $$\\frac{\\partial f}{\\partial y} = ## [REQUIRED]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I0yUajgajWJ"
      },
      "outputs": [],
      "source": [
        "a, b = symbols('x, y')\n",
        "print('Our cost function f(x, y) is: ', f(a, b))\n",
        "print('Partial derivative wrt x is: ', diff(f(a, b), b))\n",
        "print('Value of f(x,y) at x=1.8 y=1.0 is: ',\n",
        "      f(a, b).evalf(subs={a:1.8, b:1.0})) # Python Dictionary\n",
        "print('Value of partial derivative wrt x: ', diff(f(a, b), a).evalf(subs={a:1.8, b:1.0}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O5Gwd-FajWJ"
      },
      "source": [
        "#### Batch Gradient Descent with SymPy\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSAXXJErajWJ"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "multiplier = 0.1\n",
        "max_iter = 500\n",
        "params = np.array([1.8, 1.0]) # initial guess\n",
        "\n",
        "for n in range(max_iter):\n",
        "    gradient_x = diff(f(a, b), a).evalf(subs={a:params[0], b:params[1]})\n",
        "    gradient_y = diff(f(a, b), b).evalf(subs={a:params[0], b:params[1]})\n",
        "    gradients = np.array([gradient_x, gradient_y])\n",
        "    params = params - multiplier * gradients\n",
        "\n",
        "# Results\n",
        "print('Values in gradient array', gradients)\n",
        "print('Minimum occurs at x value of: ', params[0])\n",
        "print('Minimum occurs at y value of: ', params[1])\n",
        "print('The cost is: ', f(params[0], params[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "ZRVIT0gtajWJ"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "##########################\n",
        "\n",
        "## TASK-10 : Complete the lines of code wherever marked as [REQUIRED] in this cell.\n",
        "\n",
        "##########################\n",
        "##########################\n",
        "\n",
        "# Partial derivative functions example 4\n",
        "def fpx(x, y):\n",
        "    return ## [REQUIRED]\n",
        "\n",
        "def fpy(x, y):\n",
        "    return ## [REQUIRED]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3cgTnHQajWK"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "multiplier = 0.1\n",
        "max_iter = 500\n",
        "params = np.array([1.8, 1.0]) # initial guess\n",
        "\n",
        "for n in range(max_iter):\n",
        "    gradient_x = fpx(params[0], params[1])\n",
        "    gradient_y = fpy(params[0], params[1])\n",
        "    gradients = np.array([gradient_x, gradient_y])\n",
        "    params = params - multiplier * gradients\n",
        "\n",
        "# Results\n",
        "print('Values in gradient array', gradients)\n",
        "print('Minimum occurs at x value of: ', params[0])\n",
        "print('Minimum occurs at y value of: ', params[1])\n",
        "print('The cost is: ', f(params[0], params[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W3bN5bnajWK"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "#### This is formatted as code\n",
        "```\n",
        "\n",
        "### Graphing 3D Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elxOfTHfajWK"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "multiplier = 0.1\n",
        "max_iter = 200\n",
        "params = np.array([1.8, 1.0]) # initial guess\n",
        "values_array = params.reshape(1, 2)\n",
        "print(values_array.shape)\n",
        "\n",
        "for n in range(max_iter):\n",
        "    gradient_x = fpx(params[0], params[1])\n",
        "    gradient_y = fpy(params[0], params[1])\n",
        "    gradients = np.array([gradient_x, gradient_y])\n",
        "    params = params - multiplier * gradients\n",
        "    #values_array = np.append(values_array, params.reshape(1, 2), axis=0)\n",
        "    values_array = np.concatenate((values_array, params.reshape(1, 2)), axis=0)\n",
        "\n",
        "\n",
        "# Results\n",
        "print('Values in gradient array', gradients)\n",
        "print('Minimum occurs at x value of: ', params[0])\n",
        "print('Minimum occurs at y value of: ', params[1])\n",
        "print('The cost is: ', f(params[0], params[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wouc2U-wajWK"
      },
      "outputs": [],
      "source": [
        "# Generating 3D Plot\n",
        "fig = plt.figure(figsize=[16, 12])\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "ax.set_xlabel('X', fontsize=20)\n",
        "ax.set_ylabel('Y', fontsize=20)\n",
        "ax.set_zlabel('f(x, y) - Cost', fontsize=20)\n",
        "\n",
        "ax.plot_surface(x_4, y_4, f(x_4, y_4), cmap=cm.coolwarm, alpha=0.4)\n",
        "ax.scatter(values_array[:, 0], values_array[:, 1],\n",
        "           f(values_array[:, 0], values_array[:, 1]), s=50, color='red')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WogkvYPPajWK"
      },
      "source": [
        "#### For Reference, practice and play with the code below\n",
        "\n",
        "### Example 5 - Working with data & a real cost function\n",
        "\n",
        "#### Mean Squared Error: a cost function for regression problems\n",
        "\n",
        "#### $$RSS = \\sum_{i=1}^{n} \\big( y^{(i)} - h_\\theta x^{(i)} \\big)^2 $$\n",
        "#### $$MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\big( y^{(i)} - h_\\theta x^{(i)} \\big)^2 $$\n",
        "#### $$MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\big( y - \\hat{y} \\big)^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssY_aWc_ajWL"
      },
      "outputs": [],
      "source": [
        "# Make sample data\n",
        "x_5 = np.array([[0.1, 1.2, 2.4, 3.2, 4.1, 5.7, 6.5]]).transpose()\n",
        "y_5 = np.array([1.7, 2.4, 3.5, 3.0, 6.1, 9.4, 8.2]).reshape(7, 1)\n",
        "\n",
        "print('Shape of x_5 array:', x_5.shape)\n",
        "print('Shape of y_5 array:', y_5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zctpQX0ajWL"
      },
      "outputs": [],
      "source": [
        "# Quick linear regressino\n",
        "regr = LinearRegression()\n",
        "regr.fit(x_5, y_5)\n",
        "print('Theta 0:', regr.intercept_[0])\n",
        "print('Theta 1:', regr.coef_[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm67qjFkajWL"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x_5, y_5, s=50)\n",
        "plt.plot(x_5, regr.predict(x_5), color='orange', linewidth=3)\n",
        "plt.xlabel('x values')\n",
        "plt.ylabel('y values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZN6QRamajWN"
      },
      "outputs": [],
      "source": [
        "# y_hat = theta0 + theta1*x\n",
        "y_hat = 0.847535148603 + 1.22272646378*x_5\n",
        "print('Est values y_hat are: \\n', y_hat)\n",
        "print('In comparison, the actual y values are \\n', y_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "s5GfrDdiajWN"
      },
      "outputs": [],
      "source": [
        "# Challenge: Write a python function mse(y, y_hat) returns the MSE? Call the mse(y, y_hat)\n",
        "# function and print out the MSE for the y_hat calculated above.\n",
        "\n",
        "def mse(y, y_hat):\n",
        "    #mse_calc = 1/7 * sum((y - y_hat)**2)\n",
        "    #mse_calc = (1/y.size) * sum((y - y_hat)**2)\n",
        "    mse_calc = np.average((y - y_hat)**2, axis=0)\n",
        "    return mse_calc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCkldeL3ajWO"
      },
      "outputs": [],
      "source": [
        "print('Manually calculated MSE is:', mse(y_5, y_hat))\n",
        "print('MSE regression using manual calc is', mean_squared_error(y_5, y_hat))\n",
        "print('MSE regression is', mean_squared_error(y_5, regr.predict(x_5)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ_PNfYMajWO"
      },
      "source": [
        "#### 3D Plot for the MSE Cost Function\n",
        "\n",
        "#### Make data for thetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Nhnj223yajWO"
      },
      "outputs": [],
      "source": [
        "nr_thetas = 200\n",
        "th_0 = np.linspace(start=-1, stop=3, num=nr_thetas)\n",
        "th_1 = np.linspace(start=-1, stop=3, num=nr_thetas)\n",
        "plot_t0, plot_t1 = np.meshgrid(th_0, th_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV0sXwL0ajWO"
      },
      "source": [
        "#### Calc MSE using nested for loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIGZiaFVajWO"
      },
      "outputs": [],
      "source": [
        "plot_cost = np.zeros((nr_thetas, nr_thetas))\n",
        "\n",
        "for i in range(nr_thetas):\n",
        "    for j in range(nr_thetas):\n",
        "        #print(plot_t0[j][i])\n",
        "        y_hat = plot_t0[i][j] + plot_t1[i][j]*x_5\n",
        "        plot_cost[i][j] = mse(y_5, y_hat)\n",
        "\n",
        "print('Shape of plot_t0', plot_t0.shape)\n",
        "print('Shape of plot_t1', plot_t1.shape)\n",
        "print('Shape of plot_cost', plot_cost.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPFsapGEajWO"
      },
      "outputs": [],
      "source": [
        "# Plotting MSE\n",
        "fig = plt.figure(figsize=[16, 12])\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "ax.set_xlabel('Theta 0', fontsize=20)\n",
        "ax.set_ylabel('Theta 1', fontsize=20)\n",
        "ax.set_zlabel('Cost - MSE', fontsize=20)\n",
        "\n",
        "ax.plot_surface(plot_t0, plot_t1, plot_cost, cmap=cm.hot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB8x8BFdajWP"
      },
      "outputs": [],
      "source": [
        "print('Min value of plot_cost', plot_cost.min())\n",
        "ij_min = np.unravel_index(indices=plot_cost.argmin(), dims=plot_cost.shape)\n",
        "print('Min occurs at (i,j):', ij_min)\n",
        "print('Min MSE for Theta 0 at plot_t0[111][91]', plot_t0[111][91])\n",
        "print('Min MSE for Theta 1 at plot_t1[111][91]', plot_t1[111][91])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5aHbPUkajWP"
      },
      "source": [
        "#### Partial Derivatives of MSE w.r.t. $\\theta_0$ and $\\theta_1$\n",
        "\n",
        "#### $$\\frac{\\partial MSE}{\\partial \\theta_0} = - \\frac{2}{n} \\sum_{i=1}^{n} \\big( y^{(i)} - \\theta_0 - \\theta_1 x^{(i)} \\big)$$\n",
        "\n",
        "#### $$\\frac{\\partial MSE}{\\partial \\theta_1} = - \\frac{2}{n} \\sum_{i=1}^{n} \\big( y^{(i)} - \\theta_0 - \\theta_1 x^{(i)} \\big) \\big( x^{(i)} \\big)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnKlvxtFajWP"
      },
      "source": [
        "#### MSE & Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "9XPaTRrGajWP"
      },
      "outputs": [],
      "source": [
        "# x values, y values, array of theta parameters (theta0 at index 0 and theta1 at index 1)\n",
        "def grad(x, y, thetas):\n",
        "    n = y.size\n",
        "\n",
        "    # Challenge: Create theta0_slope and theta1_slope to hold slope values from partial derivs\n",
        "    theta0_slope = (-2/n) * sum(y - thetas[0] - thetas[1]*x)\n",
        "    theta1_slope = (-2/n) * sum((y - thetas[0] - thetas[1]*x)*x)\n",
        "\n",
        "    #return np.array([theta0_slope[0], theta1_slope[0]])\n",
        "    #return np.append(arr=theta0_slope, values=theta1_slope)\n",
        "    return np.concatenate((theta0_slope, theta1_slope), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA3he2YQajWP"
      },
      "outputs": [],
      "source": [
        "multiplier = 0.01\n",
        "thetas = np.array([2.9, 2.9])\n",
        "\n",
        "# Collect data points for scatter plot\n",
        "plot_vals = thetas.reshape(1, 2)\n",
        "mse_vals = mse(y_5, thetas[0] + thetas[1]*x_5)\n",
        "\n",
        "for i in range(1000):\n",
        "    thetas = thetas - multiplier * grad(x_5, y_5, thetas)\n",
        "\n",
        "    # Append the new values to our numpy arrays\n",
        "    plot_vals = np.concatenate((plot_vals, thetas.reshape(1, 2)), axis=0)\n",
        "    mse_vals = np.append(arr=mse_vals, values=mse(y_5, thetas[0] + thetas[1]*x_5))\n",
        "\n",
        "# Results\n",
        "print('Min occurs at Theta 0:', thetas[0])\n",
        "print('Min occurs at Theta 1:', thetas[1])\n",
        "print('MSE is:', mse(y_5, thetas[0] + thetas[1]*x_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxTYJAH_ajWP"
      },
      "outputs": [],
      "source": [
        "# Plotting MSE\n",
        "fig = plt.figure(figsize=[16, 12])\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "ax.set_xlabel('Theta 0', fontsize=20)\n",
        "ax.set_ylabel('Theta 1', fontsize=20)\n",
        "ax.set_zlabel('Cost - MSE', fontsize=20)\n",
        "\n",
        "ax.scatter(plot_vals[:, 0], plot_vals[:, 1], mse_vals, s=80, color='black')\n",
        "ax.plot_surface(plot_t0, plot_t1, plot_cost, cmap=cm.rainbow, alpha=0.4)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "k5aHbPUkajWP"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}